{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annykay/NoiseInDataImpact/blob/main/DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4604ba",
      "metadata": {
        "id": "cb4604ba"
      },
      "source": [
        "# Data preprocessing\n",
        "<br>\n",
        "Data preparation (including feature selection and modification, outliers dropout, etc. as well as ML classification models' hyperparameters optimization) steps are presented here. The main goal of this section is to generate dataframes and optimal models that would be further used for noise introduction simulations.\n",
        "<br>\n",
        "<br>\n",
        "There are 4 different datasets and 5 ML models (DecisionTree, RandomForest, XGB, LogisticRegression, KNearestNeighbors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1bf52f17",
      "metadata": {
        "id": "1bf52f17"
      },
      "outputs": [],
      "source": [
        "# Imports (ADD HERE IF STH NEEDED FOR YOU IS STILL ABSENT)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, PredefinedSplit\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5067dc",
      "metadata": {
        "id": "2c5067dc"
      },
      "source": [
        "### Stellar classification Dataset\n",
        "Heavenly body type classification by its spectral properties<br>\n",
        "Source: https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17\n",
        "<br>\n",
        "Preparation pipeline is partially based on: https://www.kaggle.com/code/beyzanks/stellar-classification-98-4-acc-100-auc/notebook<br><br>\n",
        "**General info:**<br>\n",
        "*Size:* 51000<br>\n",
        "*Variables type:* quantitative<br>\n",
        "*Number of classes:* 3<br>\n",
        "*Classes balance:* 1.1 / 1 / 1<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c83b72b",
      "metadata": {
        "id": "9c83b72b"
      },
      "outputs": [],
      "source": [
        "# Loading the raw data and changing class labels to integers\n",
        "\n",
        "df = pd.read_csv(r'C:\\\\Users\\gangs\\Downloads\\star_classification.csv', engine='python')\n",
        "df[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in df[\"class\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c39433b",
      "metadata": {
        "id": "2c39433b"
      },
      "outputs": [],
      "source": [
        "# Deleting outliers\n",
        "\n",
        "clf = LocalOutlierFactor()\n",
        "y_pred = clf.fit_predict(df)\n",
        "x_score = clf.negative_outlier_factor_\n",
        "outlier_score = pd.DataFrame()\n",
        "outlier_score[\"score\"] = x_score\n",
        "threshold2 = -1.5                                            \n",
        "filtre2 = outlier_score[\"score\"] < threshold2\n",
        "outlier_index = outlier_score[filtre2].index.tolist()\n",
        "df.drop(outlier_index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c156fb70",
      "metadata": {
        "id": "c156fb70"
      },
      "outputs": [],
      "source": [
        "# Deleting uncorrelated with target variable features\n",
        "\n",
        "df = df.drop(['obj_ID','alpha','delta','run_ID','rerun_ID','cam_col','field_ID','fiber_ID'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce83eeb",
      "metadata": {
        "id": "1ce83eeb"
      },
      "outputs": [],
      "source": [
        "# Downsampling of major class to make data balanced\n",
        "\n",
        "df_0 = df[df['class']==0]\n",
        "df_1 = df[df['class']==1]\n",
        "df_2 = df[df['class']==2]\n",
        "\n",
        "df_0_downsampled = resample(df_0, replace=False, n_samples=17000, random_state=123)\n",
        "df_downsampled = pd.concat([df_0_downsampled, df_1, df_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c9f495",
      "metadata": {
        "id": "26c9f495"
      },
      "outputs": [],
      "source": [
        "stellar_x = df_downsampled.drop(['class'], axis = 1)\n",
        "stellar_y = df_downsampled.loc[:,'class'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a79a1a",
      "metadata": {
        "id": "c8a79a1a"
      },
      "outputs": [],
      "source": [
        "# Data scaling and train-test splitting\n",
        "\n",
        "stellar_x = StandardScaler().fit_transform(stellar_x)\n",
        "X_train, X_test, y_train, y_test = train_test_split(stellar_x, stellar_y, test_size = 0.3, random_state = 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88251c16",
      "metadata": {
        "id": "88251c16",
        "outputId": "ef385810-4f19-4884-947d-fee6805fea23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier\n",
            "best parameters: {'max_depth': 14, 'n_estimators': 90}\n",
            "Train score: 0.9925838860437131\n",
            "Test score: 0.9770812928501469\n",
            "\n",
            "DecisionTree\n",
            "best parameters: {'max_depth': 9}\n",
            "Train score: 0.9817255757982817\n",
            "Test score: 0.9714005876591577\n",
            "\n",
            "KNearestNeigbors\n",
            "best parameters: {'n_neighbors': 3}\n",
            "Train score: 0.9650463157305572\n",
            "Test score: 0.9463271302644466\n",
            "\n",
            "XGBoostClassifier\n",
            "best parameters: {'learning_rate': 0.1, 'max_depth': 12, 'n_estimators': 100}\n",
            "Train score: 0.9993563372792656\n",
            "Test score: 0.9756447926869083\n",
            "\n",
            "Logisticregression\n",
            "best parameters: {'C': 2010}\n",
            "Train score: 0.9584417765091092\n",
            "Test score: 0.9598432908912831\n"
          ]
        }
      ],
      "source": [
        "# Models optimization\n",
        "\n",
        "RandomForest = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=0),\n",
        "    {'max_depth': [i for i in range(12, 15)], 'n_estimators': [80, 90, 100]},\n",
        "    scoring = 'f1_micro',\n",
        "    cv = 3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "RandomForest.fit(X_train, y_train)\n",
        "print('RandomForestClassifier\\nbest parameters:', RandomForest.best_params_)\n",
        "print('Train score: {}\\nTest score: {}'.format(\n",
        "    f1_score(y_train, RandomForest.predict(X_train), average='micro'),\n",
        "    f1_score(y_test, RandomForest.predict(X_test), average='micro')\n",
        "    )\n",
        ")\n",
        "\n",
        "DecisionTree = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=0),\n",
        "    {'max_depth': [i for i in range(7, 11)]},\n",
        "    scoring = 'f1_micro',\n",
        "    cv = 3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "DecisionTree.fit(X_train, y_train)\n",
        "print('\\nDecisionTree\\nbest parameters:', DecisionTree.best_params_)\n",
        "print('Train score: {}\\nTest score: {}'.format(\n",
        "    f1_score(y_train, DecisionTree.predict(X_train), average='micro'),\n",
        "    f1_score(y_test, DecisionTree.predict(X_test), average='micro')\n",
        "    )\n",
        ")\n",
        "\n",
        "KNN = GridSearchCV(\n",
        "    KNeighborsClassifier(),\n",
        "    {'n_neighbors': [i for i in range(1, 5)]},\n",
        "    scoring = 'f1_micro',\n",
        "    cv = 3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "KNN.fit(X_train, y_train)\n",
        "print('\\nKNearestNeigbors\\nbest parameters:', KNN.best_params_)\n",
        "print('Train score: {}\\nTest score: {}'.format(\n",
        "    f1_score(y_train, KNN.predict(X_train), average='micro'),\n",
        "    f1_score(y_test, KNN.predict(X_test), average='micro')\n",
        "    )\n",
        ")\n",
        "\n",
        "XGB = GridSearchCV(\n",
        "    XGBClassifier(random_state=0),\n",
        "    {'max_depth': [15, 10, 12], 'n_estimators': [100, 80, 120], 'learning_rate': [0.1, 0.2, 0.05]},\n",
        "    cv = 3,\n",
        "    scoring='f1_micro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "XGB.fit(X_train, y_train)\n",
        "print('\\nXGBoostClassifier\\nbest parameters:', XGB.best_params_)\n",
        "print('Train score: {}\\nTest score: {}'.format(\n",
        "    f1_score(y_train, XGB.predict(X_train), average='micro'),\n",
        "    f1_score(y_test, XGB.predict(X_test), average='micro')\n",
        "    )\n",
        ")\n",
        "\n",
        "LogReg = GridSearchCV(\n",
        "    LogisticRegression(random_state=0, max_iter=1000),\n",
        "    {'C': np.arange(1900, 2200, 10)},\n",
        "    cv = 3,\n",
        "    scoring='f1_micro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "LogReg.fit(X_train, y_train)\n",
        "print('\\nLogisticregression\\nbest parameters:', LogReg.best_params_)\n",
        "print('Train score: {}\\nTest score: {}'.format(\n",
        "    f1_score(y_train, LogReg.predict(X_train), average='micro'),\n",
        "    f1_score(y_test, LogReg.predict(X_test), average='micro')\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rice type classification Dataset\n",
        "Determination of rice sort (Jasmine - 1, Gonen - 0) based on its quantitative characteristics (area, axis lenght, etc.).\n",
        "<br>\n",
        "Source: https://www.kaggle.com/mssmartypants/rice-type-classification\n",
        "<br><br>\n",
        "**General info:**<br>\n",
        "*Size:* 18000<br>\n",
        "*Variables type:* quantitative<br>\n",
        "*Number of classes:* 2<br>\n",
        "*Classes balance:* 1.2 / 1<br>\n",
        "<br>\n",
        "According to kaggle description, this dataset was preproseed before and does not require any additional operations on it.<br><br>\n",
        "ML models show sufficient preformance with default parameters."
      ],
      "metadata": {
        "id": "FzTXQW10t2DZ"
      },
      "id": "FzTXQW10t2DZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pulsars classification Dataset\n",
        "Pulsars prediction by using neutron star pulse profile<br>\n",
        "Source: https://www.kaggle.com/colearninglounge/predicting-pulsar-starintermediate<br><br>\n",
        "**General info:**<br>\n",
        "*Size:* 18000<br>\n",
        "*Variables type:* quantitative<br>\n",
        "*Number of classes:* 2<br>\n",
        "*Classes balance:* 9.9 / 1<br>"
      ],
      "metadata": {
        "id": "X6tAlyTC1DL5"
      },
      "id": "X6tAlyTC1DL5"
    },
    {
      "cell_type": "code",
      "source": [
        "# We will define some functions that will automatize data downloading, processing and hyperparameters optimization\n",
        "\n",
        "DATASETS = [\n",
        "    {\n",
        "        'name': 'Pulsars',\n",
        "        'features': 'Pulsars_features.csv',\n",
        "        'labels': 'Pulsars_labels.csv'\n",
        "    }\n",
        "]   \n",
        "\n",
        "clfs = {\n",
        "    'DTC': DecisionTreeClassifier,\n",
        "    'RFC': RandomForestClassifier,\n",
        "    'LTC': LogisticRegression,\n",
        "    'XGB': XGBClassifier,\n",
        "    'KNC': KNeighborsClassifier,\n",
        "}\n",
        "\n",
        "grd_pars = {\n",
        "    'max_depth': [5, 8, 15, 30],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_features': [2, 5, 8, 12, 16, 20, 25, None],\n",
        "    'n_estimators': [20, 40, 60, 80],\n",
        "    'n_neighbors': [1, 3, 6, 9, 12, 15],\n",
        "    'learning_rate': [0.1, 0.3, 0.5, 0.7, 1],\n",
        "    'C': [ 0.5, 1, 5, 10 ],\n",
        "}\n",
        "\n",
        "mdl_pars = [\n",
        "     ['max_depth', 'criterion'],\n",
        "     ['max_depth', 'max_features', 'n_estimators'],\n",
        "     ['C',],\n",
        "     ['max_depth', 'n_estimators', 'learning_rate'],\n",
        "     ['n_neighbors',],\n",
        "]\n"
      ],
      "metadata": {
        "id": "WISO3QQh5IfH"
      },
      "id": "WISO3QQh5IfH",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_categorical(dataset):\n",
        "    features = []\n",
        "    for col in dataset.columns:\n",
        "        vals = np.sort(dataset[col].unique())\n",
        "        if np.any(vals != np.array([0,1])):\n",
        "            features.append(col)\n",
        "    return features\n",
        "\n",
        "def scale_data(X):\n",
        "    nc_cols = non_categorical(X)\n",
        "    sc = StandardScaler()\n",
        "    X_s = pd.DataFrame(sc.fit_transform(X[nc_cols]), columns=nc_cols)\n",
        "    return X_s\n",
        "\n",
        "def best_params(X, y, clf, grid):\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=0xC0FFEE)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=0xC0FFEE)\n",
        "\n",
        "    val_fold = 1 - 2*np.isin(X_train_val, X_val).all(axis=1).astype(int)\n",
        "    ps = PredefinedSplit(val_fold)\n",
        "    \n",
        "    grd_clf = GridSearchCV(clf(), grid,\n",
        "                  scoring = 'f1_micro', cv = ps, n_jobs=-1)\n",
        "\n",
        "    grd_clf.fit(X_train_val, y_train_val)\n",
        "    y_train_pred = grd_clf.best_estimator_.predict(X_train)\n",
        "\n",
        "    grd_clf.best_estimator_.fit(X_train, y_train)\n",
        "    y_test_pred = grd_clf.best_estimator_.predict(X_test)\n",
        "\n",
        "    train_score = round(f1_score(y_train, y_train_pred, average='weighted'), 2)\n",
        "    test_score = round(f1_score(y_test, y_test_pred, average='weighted'), 2)\n",
        "    \n",
        "    return train_score, test_score, grd_clf.best_params_\n",
        "\n",
        "mdl_vals = {}\n",
        "\n",
        "names, models = list(clfs.keys()), list(clfs.values())\n",
        "for model, clf, params in tqdm(zip(names, models, mdl_pars), total=len(mdl_pars)):\n",
        "    mdl_vals[model] = {key: [] for key in params}\n",
        "    for DATA in DATASETS:\n",
        "        X = pd.read_csv(DATA['features']).drop('Unnamed: 0', axis=1)\n",
        "        y = pd.read_csv(DATA['labels']).drop('Unnamed: 0', axis=1)\n",
        "        \n",
        "        X_s = scale_data(X)\n",
        "\n",
        "        grid = { key: grd_pars[key] for key in params }\n",
        "        train_score, test_score, parameters = best_params(X_s, y, clf, grid)\n",
        "        \n",
        "        print(model, DATA['name'], train_score, test_score)\n",
        "        print(parameters)\n",
        "        \n",
        "        for key in parameters:\n",
        "            mdl_vals[model][key].append(parameters[key])\n",
        "        \n",
        "print(mdl_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMXkIBN7L86a",
        "outputId": "56f4f5cc-13b7-480d-cda0-7800dd8670eb"
      },
      "id": "sMXkIBN7L86a",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:00<00:01,  3.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTC Pulsars 0.98 0.97\n",
            "{'criterion': 'entropy', 'max_depth': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:16<00:29,  9.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFC Pulsars 0.98 0.98\n",
            "{'max_depth': 5, 'max_features': None, 'n_estimators': 40}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:16<00:10,  5.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LTC Pulsars 0.98 0.98\n",
            "{'C': 10}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:23<00:05,  5.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB Pulsars 1.0 0.98\n",
            "{'learning_rate': 0.7, 'max_depth': 15, 'n_estimators': 80}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:24<00:00,  4.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNC Pulsars 0.98 0.97\n",
            "{'n_neighbors': 3}\n",
            "{'DTC': {'max_depth': [5], 'criterion': ['entropy']}, 'RFC': {'max_depth': [5], 'max_features': [None], 'n_estimators': [40]}, 'LTC': {'C': [10]}, 'XGB': {'max_depth': [15], 'n_estimators': [80], 'learning_rate': [0.7]}, 'KNC': {'n_neighbors': [3]}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Banking classification Dataset\n",
        "Prediction of client subscription (y/n) for term deposit<br>\n",
        "Source: https://www.kaggle.com/rashmiranu/banking-dataset-classification<br><br>\n",
        "**General info:**<br>\n",
        "*Size:* 33000<br>\n",
        "*Variables type:* categorical<br>\n",
        "*Number of classes:* 2<br>\n",
        "*Classes balance:* 9.7 / 1<br>"
      ],
      "metadata": {
        "id": "Dr9Bqd46aG6P"
      },
      "id": "Dr9Bqd46aG6P"
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use previously defined functions to estimate the models' parameters\n",
        "\n",
        "DATASETS = [\n",
        "    {\n",
        "        'name': 'Banking',\n",
        "        'features': 'bank_data_features.csv',\n",
        "        'labels': 'bank_data_labels.csv'\n",
        "    }\n",
        "]  \n",
        "\n",
        "mdl_vals = {}\n",
        "\n",
        "names, models = list(clfs.keys()), list(clfs.values())\n",
        "for model, clf, params in tqdm(zip(names, models, mdl_pars), total=len(mdl_pars)):\n",
        "    mdl_vals[model] = {key: [] for key in params}\n",
        "    for DATA in DATASETS:\n",
        "        X = pd.read_csv(DATA['features'])\n",
        "        y = pd.read_csv(DATA['labels'])\n",
        "        \n",
        "        X_s = scale_data(X)\n",
        "\n",
        "        grid = { key: grd_pars[key] for key in params }\n",
        "        train_score, test_score, parameters = best_params(X_s, y, clf, grid)\n",
        "        \n",
        "        print(model, DATA['name'], train_score, test_score)\n",
        "        print(parameters)\n",
        "        \n",
        "        for key in parameters:\n",
        "            mdl_vals[model][key].append(parameters[key])\n",
        "        \n",
        "print(mdl_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf-BrHdCdIcs",
        "outputId": "526ed006-f058-4511-b0b1-dd1a29f48a07"
      },
      "id": "sf-BrHdCdIcs",
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:01<00:06,  1.50s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DTC Banking 0.91 0.89\n",
            "{'criterion': 'gini', 'max_depth': 8}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:45<01:19, 26.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RFC Banking 0.91 0.89\n",
            "{'max_depth': 8, 'max_features': 2, 'n_estimators': 20}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:46<00:29, 14.64s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LTC Banking 0.89 0.89\n",
            "{'C': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [02:11<00:42, 42.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB Banking 0.91 0.89\n",
            "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 80}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [02:13<00:00, 26.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNC Banking 0.9 0.89\n",
            "{'n_neighbors': 15}\n",
            "{'DTC': {'max_depth': [8], 'criterion': ['gini']}, 'RFC': {'max_depth': [8], 'max_features': [2], 'n_estimators': [20]}, 'LTC': {'C': [0.5]}, 'XGB': {'max_depth': [5], 'n_estimators': [80], 'learning_rate': [0.1]}, 'KNC': {'n_neighbors': [15]}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "DataPreprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}